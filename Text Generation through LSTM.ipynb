{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ea6a1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.nn.utils import clip_grad_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4af17938",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # GPU set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bfe2589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dictionary \n",
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}    # word to index, key: word; value: index\n",
    "        self.idx2word = {}    # index to word, key: index; value:word\n",
    "        self.idx = 0\n",
    "        \n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5af946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Proccessing\n",
    "class TextProcess(object):\n",
    "    def __init__(self):\n",
    "        self.dictionary = Dictionary()\n",
    "        \n",
    "    def get_data(self, path, batch_size = 20):\n",
    "        with open(path, 'r') as f:\n",
    "            tokens = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                tokens += len(words)\n",
    "                for word in words:\n",
    "                    self.dictionary.add_word(word)\n",
    "                \n",
    "        # Create 1D tensor that contains the index of all the words in the file\n",
    "        rep_tensor = torch.LongTensor(tokens)\n",
    "        index = 0\n",
    "        with open(path, 'r') as f:\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                for word in words:\n",
    "                    rep_tensor[index] = self.dictionary.word2idx[word]\n",
    "                    index += 1\n",
    "        # find out how many batch we need\n",
    "        num_batches = rep_tensor.shape[0] // batch_size\n",
    "        # remove the remainder (filter out the ones that don't fit)\n",
    "        rep_tensor = rep_tensor[:num_batches * batch_size]\n",
    "        rep_tensor = rep_tensor.view(batch_size, -1)   # (batch_size, num_batches)\n",
    "        return rep_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c902445c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ************   change here For new data set, put .txt dataset in same folder *********************\n",
    "# **************************************************************************************************\n",
    "\n",
    "# Defining Parameter \n",
    "embedd_size = 128    # word is getting embedding 128 dimension vector\n",
    "hidden_size = 1024    # hidden neural of each layer\n",
    "num_layers = 2     # Number of LSTM layer\n",
    "num_epochs = 100   # Number of training epochs\n",
    "batch_size = 20\n",
    "timesteps = 30      # Consider 30 timestep to predict next word\n",
    "learning_rate = 0.002 \n",
    "path = 'alice.txt'  # Corpus Dataset path\n",
    "\n",
    "# *************************************************************************************************\n",
    "# ****************   only change here   ***********************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95c176d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size shape: torch.Size([20, 1484])\n",
      "Vocabulary size: 5290\n",
      "Number of batches: 49\n"
     ]
    }
   ],
   "source": [
    "# create corpus\n",
    "corpus = TextProcess()\n",
    "\n",
    "# set represented Tensor, vocabulary Size and Number of Batchees\n",
    "rep_tensor = corpus.get_data(path, batch_size)\n",
    "vocab_size = len(corpus.dictionary)\n",
    "num_batches = rep_tensor.shape[1] // timesteps\n",
    "\n",
    "print('Batch size shape: {}'.format(rep_tensor.shape))\n",
    "print('Vocabulary size: {}'.format(vocab_size))\n",
    "print('Number of batches: {}'.format(num_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37c613a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM model \n",
    "class TextGenerator(nn.Module):\n",
    "    def __init__(self, vocab_size, embedd_size, hidden_size, num_layers):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embedd_size)    # word transfer to 5290*128 vector\n",
    "        self.lstm = nn.LSTM(embedd_size, hidden_size, num_layers, batch_first=True)\n",
    "#         self.linear1 = nn.Linear(hidden_size, hidden_size)\n",
    "#         self.drop = nn.Dropout(0.2)\n",
    "        self.linear2 = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        # perform word embedding\n",
    "        x = self.embed(x)\n",
    "        # x = x.view(batch_size, timesteps, embedd_size)\n",
    "        out, (h, c) = self.lstm(x, h)\n",
    "        out = out.reshape(out.size(0) * out.size(1), out.size(2))  # (batch_size*timesteps, hidden_size)\n",
    "#         out = self.linear1(out)\n",
    "#         out = self.drop(out)\n",
    "        out = self.linear2(out)\n",
    "        return out, (h, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d12b891e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = TextGenerator(vocab_size, embedd_size, hidden_size, num_layers).to(device)\n",
    "# loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "#optimizer \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf8ac13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_156755/3001538218.py:22: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  clip_grad_norm(model.parameters(), 0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100]; Loss: 8.570\n",
      "Epoch [2/100]; Loss: 6.206\n",
      "Epoch [3/100]; Loss: 5.794\n",
      "Epoch [4/100]; Loss: 5.471\n",
      "Epoch [5/100]; Loss: 5.156\n",
      "Epoch [6/100]; Loss: 4.893\n",
      "Epoch [7/100]; Loss: 4.571\n",
      "Epoch [8/100]; Loss: 4.286\n",
      "Epoch [9/100]; Loss: 4.071\n",
      "Epoch [10/100]; Loss: 3.908\n",
      "Epoch [11/100]; Loss: 3.732\n",
      "Epoch [12/100]; Loss: 3.257\n",
      "Epoch [13/100]; Loss: 3.054\n",
      "Epoch [14/100]; Loss: 2.724\n",
      "Epoch [15/100]; Loss: 2.392\n",
      "Epoch [16/100]; Loss: 2.051\n",
      "Epoch [17/100]; Loss: 1.751\n",
      "Epoch [18/100]; Loss: 1.359\n",
      "Epoch [19/100]; Loss: 1.048\n",
      "Epoch [20/100]; Loss: 0.781\n",
      "Epoch [21/100]; Loss: 0.508\n",
      "Epoch [22/100]; Loss: 0.302\n",
      "Epoch [23/100]; Loss: 0.171\n",
      "Epoch [24/100]; Loss: 0.104\n",
      "Epoch [25/100]; Loss: 0.080\n",
      "Epoch [26/100]; Loss: 0.069\n",
      "Epoch [27/100]; Loss: 0.066\n",
      "Epoch [28/100]; Loss: 0.064\n",
      "Epoch [29/100]; Loss: 0.063\n",
      "Epoch [30/100]; Loss: 0.062\n",
      "Epoch [31/100]; Loss: 0.061\n",
      "Epoch [32/100]; Loss: 0.060\n",
      "Epoch [33/100]; Loss: 0.060\n",
      "Epoch [34/100]; Loss: 0.059\n",
      "Epoch [35/100]; Loss: 0.059\n",
      "Epoch [36/100]; Loss: 0.058\n",
      "Epoch [37/100]; Loss: 0.058\n",
      "Epoch [38/100]; Loss: 0.057\n",
      "Epoch [39/100]; Loss: 0.057\n",
      "Epoch [40/100]; Loss: 0.056\n",
      "Epoch [41/100]; Loss: 0.056\n",
      "Epoch [42/100]; Loss: 0.056\n",
      "Epoch [43/100]; Loss: 0.056\n",
      "Epoch [44/100]; Loss: 0.055\n",
      "Epoch [45/100]; Loss: 0.056\n",
      "Epoch [46/100]; Loss: 0.055\n",
      "Epoch [47/100]; Loss: 0.055\n",
      "Epoch [48/100]; Loss: 0.054\n",
      "Epoch [49/100]; Loss: 0.055\n",
      "Epoch [50/100]; Loss: 0.054\n",
      "Epoch [51/100]; Loss: 0.055\n",
      "Epoch [52/100]; Loss: 0.054\n",
      "Epoch [53/100]; Loss: 0.054\n",
      "Epoch [54/100]; Loss: 0.054\n",
      "Epoch [55/100]; Loss: 0.054\n",
      "Epoch [56/100]; Loss: 0.053\n",
      "Epoch [57/100]; Loss: 0.054\n",
      "Epoch [58/100]; Loss: 0.053\n",
      "Epoch [59/100]; Loss: 0.054\n",
      "Epoch [60/100]; Loss: 0.053\n",
      "Epoch [61/100]; Loss: 0.054\n",
      "Epoch [62/100]; Loss: 0.053\n",
      "Epoch [63/100]; Loss: 0.054\n",
      "Epoch [64/100]; Loss: 0.053\n",
      "Epoch [65/100]; Loss: 0.053\n",
      "Epoch [66/100]; Loss: 0.053\n",
      "Epoch [67/100]; Loss: 0.053\n",
      "Epoch [68/100]; Loss: 0.053\n",
      "Epoch [69/100]; Loss: 0.053\n",
      "Epoch [70/100]; Loss: 0.053\n",
      "Epoch [71/100]; Loss: 0.053\n",
      "Epoch [72/100]; Loss: 0.053\n",
      "Epoch [73/100]; Loss: 0.053\n",
      "Epoch [74/100]; Loss: 0.052\n",
      "Epoch [75/100]; Loss: 0.053\n",
      "Epoch [76/100]; Loss: 0.052\n",
      "Epoch [77/100]; Loss: 0.053\n",
      "Epoch [78/100]; Loss: 0.052\n",
      "Epoch [79/100]; Loss: 0.053\n",
      "Epoch [80/100]; Loss: 0.052\n",
      "Epoch [81/100]; Loss: 0.053\n",
      "Epoch [82/100]; Loss: 0.052\n",
      "Epoch [83/100]; Loss: 0.052\n",
      "Epoch [84/100]; Loss: 0.052\n",
      "Epoch [85/100]; Loss: 0.052\n",
      "Epoch [86/100]; Loss: 0.052\n",
      "Epoch [87/100]; Loss: 0.052\n",
      "Epoch [88/100]; Loss: 0.052\n",
      "Epoch [89/100]; Loss: 0.052\n",
      "Epoch [90/100]; Loss: 0.052\n",
      "Epoch [91/100]; Loss: 0.052\n",
      "Epoch [92/100]; Loss: 0.052\n",
      "Epoch [93/100]; Loss: 0.052\n",
      "Epoch [94/100]; Loss: 0.052\n",
      "Epoch [95/100]; Loss: 0.052\n",
      "Epoch [96/100]; Loss: 0.051\n",
      "Epoch [97/100]; Loss: 0.052\n",
      "Epoch [98/100]; Loss: 0.051\n",
      "Epoch [99/100]; Loss: 0.052\n",
      "Epoch [100/100]; Loss: 0.051\n"
     ]
    }
   ],
   "source": [
    "# Training the network\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    # set initial hidden and cell state\n",
    "    states = (torch.zeros(num_layers, batch_size, hidden_size).to(device),\n",
    "             torch.zeros(num_layers, batch_size, hidden_size).to(device))\n",
    "#     states = states.to(device)\n",
    "    \n",
    "    for i in range(0, rep_tensor.size(1) - timesteps, timesteps):\n",
    "        # get mini-batch input and targets\n",
    "        inputs = rep_tensor[:, i:i + timesteps].to(device)\n",
    "        targets = rep_tensor[:, (i+1):(i+1) + timesteps].to(device)\n",
    "        \n",
    "        #example sentence: ram is outstanding: \n",
    "        # input = ram is\n",
    "        # output = am is o\n",
    "        outputs, _ = model(inputs, states)\n",
    "        loss = loss_fn(outputs, targets.reshape(-1))\n",
    "        \n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        clip_grad_norm(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        \n",
    "        step = (i + 1) // timesteps\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print('Epoch [{}/{}]; Loss: {:.3f}'.format(epoch+1, num_epochs, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95251511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it muttering into the King say I know what is the words:-- \n",
      "\n",
      "\n",
      "\n",
      "'Well, I know I shan't is that Cheshire 'I suppose she had a \n",
      "\n",
      "\n",
      "\n",
      "with the Mock \n",
      "\n",
      "to the Cat; the way YOU like it would not help the moment Alice in the Mouse, \n",
      "\n",
      "'It was not here \n",
      "\n",
      "\n",
      "\n",
      "her in the beginning,' it unfolded the rest the time was nothing written and, as she had come out \n",
      "\n",
      "For \n",
      "\n",
      "'Ah! and the time you'll be sure! the top much what the silence. \n",
      "\n",
      "'Why, of them even if it say, 'For \n",
      "\n",
      "proposal. \n",
      "\n",
      "\n",
      "\n",
      "yet.' \n",
      "\n",
      "'You can catch the teacups \n",
      "\n",
      "Soon just at the words she felt very good-naturedly \n",
      "\n",
      "'Ah! upon a growl, paws. \n",
      "\n",
      "First, she had a mouse--a \n",
      "\n",
      "'There's \n",
      "\n",
      "And she's course it unfolded the \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "you never to usurpation at \n",
      "\n",
      "with his confusion of a very good-naturedly swam YOU like it would not help \n",
      "\n",
      "to the way YOU like being held Alice; 'only, \n",
      "\n",
      "that the time waited till she is \n",
      "\n",
      "ignorant \n",
      "\n",
      "growing, FATHER as a wink and the trial's \n",
      "\n",
      "cat a little feet, and, as she got so she had fallen so she could \n",
      "\n",
      "are; cat say, see, when she had fallen she ought to \n",
      "\n",
      "her usual and the hall. 'I don't see it unfolded the words she had fallen it as she had a table, \n",
      "\n",
      "Soon very grave it unfolded her saucer of it: I might catch the Gryphon. 'They told out \n",
      "\n",
      "diamonds, and the sort. and Alice looked at Alice. However, \n",
      "\n",
      "I know?' \n",
      "\n",
      "of meaning in the edge the way of conversation. \n",
      "\n",
      "moment Alice (she \n",
      "\n",
      "\n",
      "\n",
      "Five and it \n",
      "\n",
      "'I don't you may be the beginning,' because said Alice (she \n",
      "\n",
      "However, \n",
      "\n",
      "\n",
      "\n",
      "Luckily 'For the use of of?' \n",
      "\n",
      "\n",
      "\n",
      "'You may look at \n",
      "\n",
      "\n",
      "\n",
      "CHORUS. \n",
      "\n",
      "\n",
      "\n",
      "ignorant not at \n",
      "\n",
      "which were learning Alice (she \n",
      "\n",
      "she had fallen I growl \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "poor Alice! \n",
      "\n",
      "However, 'jury-men' into the beginning,' the March Hare will you, or not. \n",
      "\n",
      "'So did not help of them to watch,' \n",
      "\n",
      "'You can take no pleasing \n",
      "\n",
      "with \n",
      "\n",
      "his father; them, they walked on its voice. \n",
      "\n",
      "\n",
      "\n",
      "'What a snail. \n",
      "\n",
      "said, \n",
      "\n",
      "on treacle,' I'll \n",
      "\n",
      "saying door--I \n",
      "\n",
      "\"What \n",
      "\n",
      "'You can six you know,' said the \n",
      "\n",
      "the first was a \n",
      "\n",
      "'No, you know,' said Alice in the pig-baby watching the words:-- \n",
      "\n",
      "\n",
      "\n",
      "the time honour!' or two, looking at a mouse--a \n",
      "\n",
      "\n",
      "\n",
      "you never been \n",
      "\n",
      "'I'd a very good-naturedly the Mouse, \n",
      "\n",
      "\n",
      "\n",
      "'What rules for a snail. \n",
      "\n",
      "\n",
      "\n",
      "'What for the Mouse, \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "diamonds, and felt very good-naturedly had no pleasing \n",
      "\n",
      "\n",
      "\n",
      "jumping that had come out \n",
      "\n",
      "'She's in a wink with the teacups the confused \n",
      "\n",
      "'Oh, you know,' said Alice; 'only, \n",
      "\n",
      "said, \n",
      "\n",
      "\n",
      "\n",
      "'You can have nothing had a buttercup of a growl, child \n",
      "\n",
      "Five and the edge the roof Latin with his confusion and felt very good-naturedly was just at a small the edge the fight and Alice in the \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "'You may not join the rest the words:-- \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "'Boots with them, they walked the time are, the sort. of you know,' said the Queen of a thousand with his confusion and no pleasing \n",
      "\n",
      "nothing. \n",
      "\n",
      "which was a stay!' \n",
      "\n",
      "And she's put say I shan't \n",
      "\n",
      "'I've a table, it muttering I know?' \n",
      "\n",
      "\n",
      "\n",
      "The Mouse was nothing had a \n",
      "\n",
      "in a growl, in a thousand \n",
      "\n",
      "repeated thoughtfully. \n",
      "\n",
      "'Perhaps \n",
      "\n",
      "which was just at first was a growl, while you'll come out \n",
      "\n",
      "'Ah! on the words she had come out \n",
      "\n",
      "growing, FATHER of the \n",
      "\n",
      "\n",
      "\n",
      "with his history. and Alice (she \n",
      "\n",
      "'I don't seem Alice. \n",
      "\n",
      "\n",
      "\n",
      "jumping \n",
      "\n",
      "\n",
      "\n",
      "The Queen was a growl, or two, looking down her in a mouse--a \n",
      "\n",
      "her next witness!' \n",
      "\n",
      "of it was just at Alice. 'What CAN I might catch the March Hare will burn up towards it was nothing had no pleasing \n",
      "\n",
      "\n",
      "\n",
      "said the March Hare will you, you a \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "'--yes, I think you'll no pleasing growing, and the Duchess, up and looked at a little open \n",
      "\n",
      "'I don't \n",
      "\n",
      "you know what \n",
      "\n",
      "'Oh, his father; looked at Alice. \n",
      "\n",
      "'I'd \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "'I suppose she is that day. dear! he spoke. (The good-naturedly made of?' \n",
      "\n",
      "'What of meaning how she ought to open \n",
      "\n",
      "'--yes, Alice a \n",
      "\n",
      "moment Alice in silence. However, \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "the top sound.] \n",
      "\n",
      "with \n",
      "\n",
      "on treacle,' I'll have nothing written you know,' said with \n",
      "\n",
      "'You can lady,' it muttering I'll \n",
      "\n",
      "'Yes, and looked at a \n",
      "\n",
      "diamonds, and Alice a very good-naturedly felt very good-naturedly made \n",
      "\n",
      "right at the King say no pleasing you never said the silence. \n",
      "\n",
      "Luckily \n",
      "\n",
      "\n",
      "\n",
      "key no pleasing \n",
      "\n",
      "\n",
      "\n",
      "For \n",
      "\n",
      "Alice looked at the confused \n",
      "\n",
      "\n",
      "\n",
      "Soon out \n",
      "\n",
      "'She's in the March Hare will you, so she could not join the March Hare will burn their paws. \n",
      "\n",
      "moment Alice (she \n",
      "\n",
      "ignorant \n",
      "\n",
      "\n",
      "\n",
      "'Ah! for apples, some noise \n",
      "\n",
      "moment Alice (she \n",
      "\n",
      "\n",
      "\n",
      "desperate 'You're look at Alice. \n",
      "\n",
      "which was just at the March Hare will you, and it \n",
      "\n",
      "'You can is that did Alice in a mouse--a \n",
      "\n",
      "the words she had come out \n",
      "\n",
      "CHORUS. \n",
      "\n",
      "nothing had come out \n",
      "\n",
      "'I don't \n",
      "\n",
      "\n",
      "\n",
      "However, \n",
      "\n",
      "her in her as the March Hare will you, and was just at the King. \n",
      "\n",
      "I know?' \n",
      "\n",
      "'--yes, the way YOU like that!' rumbling at Alice. 'I've a thing,' they walked \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "his father; through the Gryphon. 'They told \n"
     ]
    }
   ],
   "source": [
    "#Testing and Generating new Text of same corpus\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    with open('results.txt', 'w') as f:\n",
    "        states = (torch.zeros(num_layers, 1, hidden_size).to(device),\n",
    "             torch.zeros(num_layers, 1, hidden_size).to(device))\n",
    "        \n",
    "        inputs = torch.randint(0, vocab_size, (1,)).long().unsqueeze(1).to(device)\n",
    "        for i in range(1000):\n",
    "            output, _ = model(inputs, states)\n",
    "#             print(output.shape)\n",
    "            prob = output.exp()\n",
    "            word_id = torch.multinomial(prob, num_samples=1).item()\n",
    "#             print(word_id)\n",
    "            inputs.fill_(word_id)\n",
    "\n",
    "            word = corpus.dictionary.idx2word[word_id]\n",
    "            word = '\\n' if word == '<eos>' else word +  ' '\n",
    "            f.write(word)\n",
    "\n",
    "#             if (i+1)%100 == 0:\n",
    "#                 print('Sample: [{}/{}] word and save to {}'.format(i+1, 500, 'result.txt'))\n",
    "                \n",
    "                \n",
    "with open('results.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0385ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
